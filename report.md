# Report

## DAY1
### 初期解析メモ

データの内容について
```
 - データの特徴
 - 「条」の次に「項」が0〜N行入る。その後に「号」が0〜n行入る
 - 第X条と始まる項目に「条」が多いが、（研究内容）「条」のあとに第X条〜という文面ではいるケースも有る。おそらく同一文書内においては、スタイルは統一される
 - 「条」は体言止めで表現される？
 - 「項」は文章あるいは、途中分割された文章となる？
 - 「項」はXXとする。XXと定める。XXする。XX出来る。ならない。ない。のような文末になる
 - 「条」の次のLineは必ず「項」になる？　->　条自体に内容が書かれているケースもある（line14015)
 -  単文が与えられたときに「項」と「号」はどうやって見分ける？
``` 

アプローチと優先順位について
```
（データの確認）
 - 「条」「項」における単語の出現頻度を確認する
 - 「条」「項」における単語の重複度合いを確認する
 （アプローチ）
 - BERTが誤って分類したものを確認する
  - それがルールベースで拾えそうかどうか確認し、出来そうなら試す
  - BERTの最終層を出力し、他の分類器に掛ける  
  - 文書ごとのスタイルを判定する分類器とBERTのclassifierを組み合わせてみる
  - BERTに追加のpre-trainで法律ドメインのMLMを学習させたあとにclassificationする
- 懸念点、注意事項
 - ひとまずなし。
 ```

## DAY7

### 設定した課題
- 契約書データから「条」「項」を判定する。Baselineからのmicro-f1およびRecallの改善を目標とする。RecallについてはWordに提示された課題内容にて、条項出でない部分を判断してしまったりするのはOKとあるため、一応の設定とし、基本はmicro-f1の改善を課題に設定する。望ましくないが、検証途中で気づいたため、時間の都合、モデルは「条」「項」「号」の判定、実験は「条」「項」の判定となっている。

Baseline
```
F1 Score (Micro) = 0.9267202859696158
R Score (Micro) = 0.9242424242424242
```
ただし数値にブレがあり再現試験では下記となった。
```
R Score (Micro) = 0.9327094474153298
F1 Score (Micro) = 0.9320863950122467
```

### 手法の選択（Day1に従う）
 1. まずは「条」「項」における単語の分布を確認し、ルールベース出来そうかを確認する
 2. BERTが誤ったデータを確認し、そのデータを元に改善点やルールベースで回収可能と考えたものを確認する
 3. 1,2の結果を元に判定するロジックを考える（今回はここまで実施）
 4. 実装・評価を行う。
 
 アプローチの順序の選定について
 - まずはデータ確認
 - 次にBaselineの再現試験
 - 上記により得られる情報があるので、それを用いて仮設を立て、検証検証においては、時間が限られているので検証速度を優先する



### 実験結果

- 詳細はnote-bookを参照
- 単語のカウント・重複度合いを調査（数字はサンプルデータでの出現回数)
  - TOP100で見ると、項のみに存在する単語として、「(1135, 'で'), (811, 'により'), (763, '，'), (708, 'なら'), (638, '前項')」などがあり、複数文を前提とした単語はほぼ入らないように思われるが、全件だと消えていることから、一部のレアケースでは、条にも含まれる場合があると推定される
  - ドキュメントのパースの問題と思われるが、項のみspan, id, classなどのタグが見られた
  - 全件で見たときに有効と思われるのが、「(523, '当該')、(124, '含む')、(78, '基づい')、(74, '除き')、(67, '生じる')、(47, 'あるいは')、(44, 'いかなる')、(32, '準ずる')、(32, 'さらに')、(31, '当たり')、(30, 'に際し')」辺りであり、これれは前記複数文を想定した場合の単語と合致する。ただし、有効と思われた「により、前項」等は全件だと除外されてしまっているため、未知の文書に対して必ずしも当てはまるとは断言できない。
  - (70, '本条')は、自身が条であるときには使わない単語であることが推察されるため、ルールベースに入れても問題がないと想定される。

  
- BERTが誤って出力したデータを調査
  - 語彙に問題があり、エラーの92％も[UNK]が入ったデータが占めている。UNKが非常に足を引っ張っていると考えられるためtokenizerを変えて、UNKをなくすことが有効と考えられる。
  - 誤って分類したものとして、 ---- が見られる。これはアノテーションが号となっているが、正解なのか確認が必要。文書全体の構成を元にアノテーションがされている場合は現状のtextのみを入力しても判定が不可能なため、追加のロジックが必要

### 考察

####  打ち手の案

コストパフォーマンスを考えて特に有効と思われる
- 前処理の文字列処理の追加
  - [UNK]となってしまう文字を確認し、日本語的に置き換えても意味が通り、語彙に含まれる文字に置き換える

低コストで試せるものから記載
- 語彙の変更（低コスト版）
  - 公開されている日本語の別のTokenizerを使ったモデルを試してみる（例えばSentencePieceベースのものやMecab+BPEのもの）
- 閾値の調整
  - 下の「活性化関数の変更」とは反するがWordの課題設定では、 `条項でない部分を条項と判断してしまったりするのはOK`とあるので、Precisionを捨ててRecallを上げるようしきい値を0.5から下げる方法も考えられる
- 活性化関数の変更
  - baselineでは活性化関数をSigmoidとしてマルチラベルの問題を解いているが、条・項・号・その他の4クラスからの他クラス分類問題と見なし活性化関数をSoftmaxにして解けば精度が上がる可能性がある
- ルールベースロジックの追加
  - 本条が入っていたら、項である等
- 前後の文を入力する
  - [CLS]前文[SEP]本文[SEP]後文[SEP]のような形式にして前後の文も同時に入力する。  ----しかない行は前後の文がわからないと分類出来ないと思われるが、それを解決できる可能性がある。
- Epoch数、バッチサイズの変更
  - Baselineではバッチサイズ16、Epoch1で学習しているが、より大きなバッチ、Epochで学習させた場合に精度が上がる可能性がある。バッチサイズを上げる場合にはLearning-rateを下げる、過学習の恐れがあるので、Testデータまたは、Trainデータの一部を使って、evalutationを行い、そのLossによる判断するのが無難と思われる
- 語彙の変更（高コスト版） 
  - 低コスト版でうまく行かなければ自前で再度ドメイン文書の語彙を加えて、Pretrainする。
  
#### 以下DAY1の仮設に従って記載

時間があれば検証可能

- Mecab等により体言止めであるかどうかを判定して、入力に加える
- BERTの最終層を出力し、他の分類器に掛ける  - BERTに追加のpre-trainで法律ドメインのMLMを学習させたあとにclassificationする

検討が必要

- 文書ごとのスタイルを判定する分類器とBERTのclassifierを組み合わせてみる
- 文の構造をを入力に加える

#### その他

バグについて
- transformersライブラリのバージョンが上がっており、提示された手順では動作しませんでした。ワークアラウンドとしてバージョンを固定しましたが`!pip install transformers==3.0.2`、アップデートして動作するように修正が望ましいと思われます。

所要時間
- データ確認（DAY1）1時間
- 検証（DAY2〜7) 5〜6時間
- レポート、リポジトリ準備等(DAY7) 2時間

計 8〜9時間弱
